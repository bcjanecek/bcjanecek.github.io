---
layout: post
title:      "My Introduction to Apache Spark"
date:       2020-03-03 00:28:02 +0000
permalink:  my_introduction_to_apache_spark
---


"Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it..." - Dan Ariely, Duke Professor of Psychology and Behavioral Economics, Duke University

It is well understood amonst both data professionals as well as the general public that the amount of data in the world is growing at an expoential clip. Our society's growing reliance and dependence on technology has fed this monster which has proven to be a goldmine given the correct skillset to extract insights from it. 

I first began to realize the limitations of software with regards to handling large datasets while working as a junior data analyst at a private oil and gas operator in Houston, TX. A common joke amonst petroleum engineers in college is that we earn a degree in Microsoft Excel and learn a little about oil and gas when appropriate. Given my Excel expertise which had been honed over four undergraduate years I entered my first job assuming that the program could handle anything I threw at it. This hope was soon shattered as I began to attempt to analyze a few million rows of nationwide well completion data only to crash the program over and over again -  actively restraining myself from tossing my laptop into the market street eight stories below my office window. Keeping my cool, I began to explore alternative solutions and eventually ended up learning how to create and query SQL databases and analyze the data in Tableau. 

Similar to my previous experience I suspect that very soon in my career as a data scientist I will come across a time when my now beloved Pandas dataframe is nearing the limits of its capacity for big data analysis. Given this inevitable fact I am now seeking to learn how to analyze data using Apache Spark. 

Apache Spark is a large scale data processing engine built to quickly query, analyze, and transform huge amounts of data. Popular libraries have been built for machine learning, stream processing, and SQL. The motivation in creating Apache Spark was to create a more efficient and easier to use verion of Google's distributed computing framework, MapReduce. 

I will use the next several blogs posts to describe what I've learned as I embark on my voyage into the realm of big data.


